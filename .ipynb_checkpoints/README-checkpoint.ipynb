{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals / steps of this project are the following:\n",
    "\n",
    "- Use the simulator to collect data of good driving behavior\n",
    "- Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "- Train and validate the model with a training and validation set\n",
    "- Test that the model successfully drives around track one without leaving the road\n",
    "- Summarize the results with a written report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Details\n",
    "\n",
    "I created a CNN using Keras that takes as input a 160x320 pixel image and predicts a steering angle. The trained model is able to drive both tracks smoothly at 9mph. My submission includes:\n",
    "- model.py -> script to create and train the model\n",
    "- drive.py -> file for driving the car in autonomous mode\n",
    "- model.hdf5 -> my trained model\n",
    "- Track1_final.mp4 -> video of the car driving the first track\n",
    "- Track2_final.mp4 -> video of the car driving the second track"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./inputimg.png \"Input\"\n",
    "[image2]: ./cropping.png \"Cropping\"\n",
    "[image3]: ./pooling.png \"Pooling\"\n",
    "\n",
    "\n",
    "## Model Architecture and Training Strategy\n",
    "\n",
    "The model.py file contains the details how I created and trained my CNN.\n",
    "\n",
    "\n",
    "### Model Architecture\n",
    "I tried out many different architectures and ended with the following high level description:\n",
    "\n",
    "Layer | Output shape | Number of Parameters\n",
    "------------ | -- | -- |\n",
    "INPUT | (batch, 160, 320, 3) | 0 |\n",
    "CROPPING | (batch, 60, 320, 3) | 0 |\n",
    "NORMALIZE | (batch, 60, 320, 3)| 0 |\n",
    "MINPOOL | (batch, 30, 160, 3) | 0 |\n",
    "MINPOOL | (batch, 30, 40, 3) | 0 |\n",
    "CONV(1x1)/CONV(3x3)/CONV(5x5) | (batch, 30, 40, 60) | 2.040 |\n",
    "MAXPOOL | (batch, 15, 20, 60) | 0 |\n",
    "BATCHNORM | (batch, 15, 20 , 60) | 240 |\n",
    "CONV | (batch, 15, 20, 20) | 10.820 |\n",
    "MAXPOOL | (batch, 7, 10, 20) | 0 |\n",
    "BATCHNORM | (batch, 7, 10, 20) | 80 |\n",
    "CONV | (batch, 7, 10, 10) | 1.810 |\n",
    "FLATTEN | (batch, 700) | 0 |\n",
    "DENSE | (batch, 50) | 35.050 |\n",
    "DENSE | (batch, 30) | 1.530 |\n",
    "OUTPUT | (batch, 1) | 31 |\n",
    "\n",
    "Total number of parameters: 51.601\n",
    "\n",
    "I used Relu-Units as non-linearities. My observation was that dropout layers did not lead to better models. So, I prevented overfitting mainly by reducing the size of the network, ending up with the above architecture. For training, I used the Adam optimizer.\n",
    "\n",
    "\n",
    "### Training Data\n",
    "\n",
    "I obtained training data by driving in the simulator. The car had three cameras. I ended up with 151.570 images from the center camera (two-third of the data is from the first track). Using cross-validation to determine a correction of the steering angle, I could also use the images from both the other cameras. Also, I flipped each image to obtain more training data and prevent any right-left steering bias. In total, this lead to 454.704 images for training and 113.682 images for validation.\n",
    "\n",
    "In total, I drove both tracks in both directions and added more data of the car steering back to the middle when starting at a side lane.\n",
    "\n",
    "\n",
    "##  Solution Approach\n",
    "\n",
    "I used 80% of the data for training and 20% for validation. I started with a pretty big model (lots of paramters, size ca. 50MB) but soon noticed that larger models take much longer to train without obtaining better validation errors. Thus, I ended up with a much smaller model (size ca. 660 KB).\n",
    "\n",
    "Example of an input image:\n",
    "![alt text][image1]\n",
    "\n",
    "\n",
    "Since we do not need the information of the top part of an image and the bottom part of an image, I used a cropping layer first. To decide which kind of cropping I used the following figure and ended up with 80 pixels from the top and 20 from the bottom.\n",
    "\n",
    "![alt text][image2]\n",
    "\n",
    "In order to reduce the input size, I applied pooling layers. I decided to use a 2x2 MinPooling layer followed by a 1x4 MinPooling layer. The 1x4 filter was used since the road is wide and contains many unnecessary information. Examples of different pooling and cropping combinations can be seen here:\n",
    "\n",
    "![alt text][image3]\n",
    "\n",
    "After a normalizing layer, I used an inception kind of structure where I applied a 1x1, a 3x3 and a 5x5 convolution to the input and merge these filters together. Then, I used two more 3x3 convolutional layers, followed by two fully connected layers and a final single neuron as output.\n",
    "\n",
    "I only trained the model for one epoch, i.e., I used early stopping to prevent further overfitting.\n",
    "\n",
    "\n",
    "## Data generator\n",
    "\n",
    "Since the dataset is very large, I used a data generator to load the batches into the RAM. The flipping and the change of the steering angle for the side camera images was done in the generator routine.\n",
    "\n",
    "\n",
    "# Simulation\n",
    "\n",
    "The simulation videos are included. The car can drive autonomously through both tracks safely with a human-like driving behavior. It tries to stay in the middle of the road."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
