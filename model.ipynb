{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#import cv2\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Import Data\n",
    "################################################################################################################\n",
    "images = []\n",
    "measurements = []\n",
    "lines0 = []\n",
    "S = np.array([0, 1, -1])\n",
    "\n",
    "for i in range(8):\n",
    "    print(i+1)\n",
    "    lines = []\n",
    "    with open('./data{}/driving_log.csv'.format(i+1)) as csvfile: # The data is organized in different folders\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:            \n",
    "            lines.append(line)\n",
    "            for k in range(3):\n",
    "                lines0.append(line)\n",
    " \n",
    "print(\"{} pictures, i.e., {} for training and {} for validation.\".format(3*len(lines0),round(0.8*3*len(lines0)),round(0.2*3*len(lines0))))\n",
    "print(\"With flipping, we have {}  images for training.\".format(round(0.8*3*2*len(lines0))))\n",
    "print(\"If we only use the center image and flipping, we have {} images for training.\".format(round(0.8*2*len(lines0))))\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Splitting data in train and validation set\n",
    "################################################################################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(lines0, test_size=0.2)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Data generator. Data augmentation is done here.\n",
    "################################################################################################################\n",
    "import sklearn\n",
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "# This function is used to identify the correct folder. Prepare for some ugly code.\n",
    "def tryout(name):\n",
    "    try:\n",
    "        cur_path = './data1/IMG/' + name\n",
    "        img = mpimg.imread(cur_path)\n",
    "    except:\n",
    "        try:\n",
    "            cur_path = './data2/IMG/' + name\n",
    "            img = mpimg.imread(cur_path)\n",
    "        except:\n",
    "            try:\n",
    "                cur_path = './data3/IMG/' + name\n",
    "                img = mpimg.imread(cur_path)\n",
    "            except:\n",
    "                try:\n",
    "                    cur_path = './data4/IMG/' + name\n",
    "                    img = mpimg.imread(cur_path)\n",
    "                except:\n",
    "                    try:\n",
    "                        cur_path = './data5/IMG/' + name\n",
    "                        img = mpimg.imread(cur_path)\n",
    "                    except:\n",
    "                        try:\n",
    "                            cur_path = './data6/IMG/' + name\n",
    "                            img = mpimg.imread(cur_path)\n",
    "                        except:\n",
    "                            try:\n",
    "                                cur_path = './data7/IMG/' + name\n",
    "                                img = mpimg.imread(cur_path)\n",
    "                            except:\n",
    "                                cur_path = './data8/IMG/' + name\n",
    "                                img = mpimg.imread(cur_path)\n",
    "    return img\n",
    "\n",
    "def generator(samples, batch_size = 32):\n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            \n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                for k in range(3):\n",
    "                    name = batch_sample[k].split('\\\\')[-1]\n",
    "                    center_image = tryout(name)\n",
    "                    center_image_flipped = np.fliplr(center_image)                                  \n",
    "                    center_angle = float(batch_sample[3]) + S[k] * 0.05 # steering angle correction\n",
    "                    images.append(center_image)\n",
    "                    angles.append(center_angle)\n",
    "                    images.append(center_image_flipped)\n",
    "                    angles.append(-center_angle)\n",
    "                \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Defining the train and validation generators\n",
    "################################################################################################################\n",
    "train_generator = generator(train_samples, batch_size = 8)\n",
    "validation_generator = generator(validation_samples, batch_size = 8)\n",
    "factor = 6 # factor by which the data increased in the generator routine\n",
    "\n",
    "\n",
    "################################################################################################################\n",
    "# Building and Training the model\n",
    "################################################################################################################\n",
    "from keras.models import Sequential\n",
    "from keras import *\n",
    "from keras.layers import Conv2D, AveragePooling2D, MaxPooling2D, Dropout, Lambda, Flatten, Dense, Reshape, Input, merge, normalization, Cropping2D\n",
    "from keras.models import Model\n",
    "\n",
    "inp = Input(shape=(160,320,3))\n",
    "layer_0  = Cropping2D(cropping=((80,20), (0,0)))(inp)\n",
    "layer_1  = Lambda(lambda x: x/127.5 - 1)(layer_0)\n",
    "layer_2  = Lambda(lambda x: -x)(layer_1)\n",
    "layer_3  = MaxPooling2D()(layer_2)\n",
    "layer_4  = MaxPooling2D((1,4))(layer_3)\n",
    "layer_5  = Lambda(lambda x: -x)(layer_4)\n",
    "conv1    = Conv2D(30,3,3, border_mode='same', activation='relu')(layer_4) #100\n",
    "conv2    = Conv2D(15,1,1, border_mode='same', activation='relu')(layer_4) #50\n",
    "conv3    = Conv2D(15,5,5, border_mode='same', activation='relu')(layer_4) #50\n",
    "merge1   = merge([conv1, conv2, conv3], mode = 'concat', concat_axis = -1)\n",
    "layer_6  = MaxPooling2D()(merge1)\n",
    "layer_6  = normalization.BatchNormalization()(layer_6)\n",
    "layer_8  = Conv2D(20,3,3, border_mode='same', activation='relu')(layer_6) #20\n",
    "layer_10 = MaxPooling2D()(layer_8)\n",
    "layer_10 = normalization.BatchNormalization()(layer_10)\n",
    "layer_10b= Conv2D(10,3,3, border_mode='same', activation='relu')(layer_10) #20\n",
    "layer_11 = Flatten()(layer_10b)\n",
    "layer_12 = Dense(50, activation='relu')(layer_11)\n",
    "layer_13 = Dense(30, activation='relu')(layer_12)\n",
    "outlayer = Dense(1)(layer_13)\n",
    "\n",
    "model = model = Model(input=inp, output=outlayer)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'mse', optimizer='adam')\n",
    "checkpointer = callbacks.ModelCheckpoint(filepath='./model_both_17.hdf5', verbose=1, save_best_only=True)\n",
    "history_object = model.fit_generator(train_generator, samples_per_epoch = factor*len(train_samples), validation_data = validation_generator, nb_val_samples = factor*len(validation_samples), nb_epoch = 10, callbacks=[checkpointer])\n",
    "\n",
    "print(history_object.history.keys())\n",
    "\n",
    "plt.plot(history_object.history['loss'])\n",
    "plt.plot(history_object.history['val_loss'])\n",
    "plt.title('model mean squared error loss')\n",
    "plt.ylabel('mean squared error loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
